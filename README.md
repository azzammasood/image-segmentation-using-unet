# Project: Image Segmentation using Unet

A U-Net is composed of an encoder and a decoder. The encoder convolves the input image, increasing channels and reducing spacial dimensions. The decoder deconvolves the image, reducing channels and increasing spatial dimensions. The tensor that is passed in the decoder is usually called bottleneck. At the end, the spatial dimensions are restored to make a prediction for each pixel in the input image. These types of models are extremely utilized in real world applications. 

U-Net architecture, the goal is to identify the boundaries of objects in an image and segment them into distinct regions. At a high level, U-Net works by first encoding the input image into a set of feature maps using a series of convolutional layers. These feature maps capture the image's important features and help identify the boundaries between different regions of the image. Next, the U-Net architecture performs a series of up-convolutions (also known as transposed convolutions or deconvolutions) to reconstruct the segmented output image. During these up-convolutions, the feature maps are combined with higher resolution feature maps from the earlier encoding layers through skip connections. These skip connections help preserve the spatial information lost during the encoding process and enable the U-Net to localize object boundaries more accurately. Finally, the output of the U-Net is a segmentation map, which is a binary mask that labels each pixel in the input image as belonging to either the object or background. The U-Net uses a softmax activation function to generate a probability distribution over the two classes (object and background) for each pixel in the input image.

The loss function that is optimized during training is typically the pixel-wise binary cross-entropy loss. The pixel-wise binary cross-entropy loss is computed by comparing the predicted segmentation map with the ground truth segmentation map on a per-pixel basis. For each pixel in the image, the loss function calculates the binary cross-entropy between the predicted probability of the pixel belonging to the object and the ground truth label (i.e., whether the pixel actually belongs to the object or not).

The binary cross-entropy loss function is a common choice for image segmentation tasks because it is well-suited for binary classification problems and is easy to optimize using gradient descent. 

Coded in Google Colab, this project features the U-Net which improves on the Fully Convolutional Network, using a somewhat similar design, but different in a few ways. Instead of one transposed convolution at the end of the network, it uses a matching number of convolutions for downsampling the input image to a feature map, and transposed convolutions for upsampling those maps back up to the original input image size. It also adds skip connections, to retain information that would otherwise become lost during encoding. Skip connections send information to every upsampling layer in the decoder from the corresponding downsampling layer in the encoder, capturing finer information while also keeping computation low. These help prevent information loss, as well as model overfitting. 

Phases:
  1. Implementing semantic image segmentation on the CARLA self-driving car dataset
  2. Applying sparse categorical crossentropy for pixelwise prediction
